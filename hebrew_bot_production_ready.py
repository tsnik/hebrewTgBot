# -*- coding: utf-8 -*-

"""
Telegram-Ð±Ð¾Ñ‚ "ÐŸÐ¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº Ð¿Ð¾ Ð¸Ð²Ñ€Ð¸Ñ‚Ñƒ"
Ð’ÐµÑ€ÑÐ¸Ñ: 14.6 (Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð° Ð»Ð¾Ð³Ð¸ÐºÐ° Ð¸Ð· v13.1)
Ð”Ð°Ñ‚Ð° Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ: 24 Ð¸ÑŽÐ»Ñ 2025 Ð³.

ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð² ÑÑ‚Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸:
- REVERT: Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð° Ð¿Ð¾Ð»Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ `normalize_hebrew` Ñ Ð»Ð¾Ð³Ð¸ÐºÐ¾Ð¹
  Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ðº Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ðµ, ÐºÐ°Ðº Ð² v13.1.
- REFACTOR: Ð’ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ `display_word_card` Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰ÐµÐ½ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ `in_dictionary`
  Ð´Ð»Ñ Ð±Ð¾Ð»ÐµÐµ ÑÐ²Ð½Ð¾Ð³Ð¾ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÐµÐ¼.
- REVERT: Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ð¹ ÑÑ‚Ð¸Ð»ÑŒ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð½ÑƒÑ‚Ñ€Ð¸
  `fetch_and_cache_word_data` Ð´Ð»Ñ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸.
- REFACTOR: Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½ Ð¿Ð°Ñ€ÑÐµÑ€ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… (`parse_noun_or_adjective_page`).
"""

import logging
import sqlite3
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup, Tag
import re
import os
import sys
from dotenv import load_dotenv
import queue
import threading
import time
from urllib.parse import quote, urljoin
from typing import Tuple, Dict, Any, List, Optional, Callable
from collections.abc import Callable as CallableABC
import asyncio

# --- Ð˜ÐœÐŸÐžÐ Ð¢Ð« TELEGRAM ---
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, Message
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    ConversationHandler,
    ContextTypes,
    filters,
)
from telegram.constants import ParseMode


# --- ÐšÐžÐÐ¤Ð˜Ð“Ð£Ð ÐÐ¦Ð˜Ð¯ Ð˜ ÐšÐžÐÐ¡Ð¢ÐÐÐ¢Ð« ---
load_dotenv()
BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip().strip("'\"")
DB_NAME = "data/hebrew_helper_cache.db"

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ð°Ñ€ÑÐµÑ€Ð° Ð¸ Ð‘Ð”
PARSING_TIMEOUT = 15
DB_READ_ATTEMPTS = 5
DB_READ_DELAY = 0.2
CONVERSATION_TIMEOUT_SECONDS = 1800 # 30 Ð¼Ð¸Ð½ÑƒÑ‚
VERB_TRAINER_RETRY_ATTEMPTS = 3

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
logging.basicConfig(format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

# --- ÐŸÐžÐ¢ÐžÐšÐžÐ‘Ð•Ð—ÐžÐŸÐÐ¡ÐÐžÐ¡Ð¢Ð¬ Ð˜ Ð‘Ð›ÐžÐšÐ˜Ð ÐžÐ’ÐšÐ˜ ---
DB_WRITE_QUEUE = queue.Queue()
PARSING_EVENTS = {}
PARSING_EVENTS_LOCK = threading.Lock()


# --- ÐÐžÐ ÐœÐÐ›Ð˜Ð—ÐÐ¦Ð˜Ð¯ Ð˜Ð’Ð Ð˜Ð¢Ð Ð˜ ÐŸÐÐ Ð¡Ð˜ÐÐ“ ÐŸÐ•Ð Ð•Ð’ÐžÐ”ÐžÐ’ ---

def normalize_hebrew(text: str) -> str:
    """
    ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÑ‚ Ñ‚ÐµÐºÑÑ‚ Ð½Ð° Ð¸Ð²Ñ€Ð¸Ñ‚Ðµ: ÑƒÐ´Ð°Ð»ÑÐµÑ‚ Ð¾Ð³Ð»Ð°ÑÐ¾Ð²ÐºÐ¸ (Ð½Ð¸ÐºÑƒÐ´) Ð¸
    Ð¿Ñ€Ð¸Ð²Ð¾Ð´Ð¸Ñ‚ Ðº Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ðµ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ.
    """
    if not text:
        return ""
    # Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð¾Ð³Ð»Ð°ÑÐ¾Ð²Ð¾Ðº (U+0591 Ð´Ð¾ U+05C7)
    text = re.sub(r'[\u0591-\u05C7]', '', text)
    # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð° ÑƒÐ½Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ (Ð¼Ð¾Ð¶Ð½Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÑ‚ÑŒ)
    # text = text.replace('×™×™', '×™')
    # text = text.replace('×•×•', '×•')
    return text.strip()

def parse_translations(raw_text: str) -> List[Dict[str, Any]]:
    """
    ÐŸÑ€Ð¸Ð½Ð¸Ð¼Ð°ÐµÑ‚ ÑÑ‹Ñ€ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ð¸Ð· div.lead Ð¸ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ ÐµÐµ Ð²
    ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð².
    """
    all_translations = []
    if not raw_text:
        return []

    major_groups = [group.strip() for group in raw_text.split(';')]

    for group_text in major_groups:
        comment_match = re.search(r'\((.*?)\)', group_text)
        comment = comment_match.group(1).strip() if comment_match else None
        
        clean_group_text = re.sub(r'\s*\((.*?)\)', '', group_text).strip()

        minor_translations = [t.strip() for t in clean_group_text.split(',')]

        for translation_text in minor_translations:
            if translation_text:
                all_translations.append({
                    'translation_text': translation_text,
                    'context_comment': comment,
                    'is_primary': False
                })

    if all_translations:
        all_translations[0]['is_primary'] = True

    return all_translations


# --- Ð£ÐŸÐ ÐÐ’Ð›Ð•ÐÐ˜Ð• Ð‘ÐÐ—ÐžÐ™ Ð”ÐÐÐÐ«Ð¥ (Ð¡ ÐŸÐžÐ”Ð”Ð•Ð Ð–ÐšÐžÐ™ Ð¢Ð ÐÐÐ—ÐÐšÐ¦Ð˜Ð™) ---

def db_worker():
    """
    Worker, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð½Ð° Ð·Ð°Ð¿Ð¸ÑÑŒ Ð² Ð‘Ð”.
    """
    os.makedirs(os.path.dirname(DB_NAME), exist_ok=True)
    conn = sqlite3.connect(DB_NAME, timeout=10)
    conn.execute("PRAGMA journal_mode=WAL;")
    while True:
        try:
            item = DB_WRITE_QUEUE.get()
            if item is None: break
            cursor = conn.cursor()
            if isinstance(item, CallableABC):
                try:
                    cursor.execute("BEGIN TRANSACTION")
                    item(cursor)
                    conn.commit()
                except Exception as e:
                    logger.error(f"DB-WORKER: ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸, Ð¾Ñ‚ÐºÐ°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼. ÐžÑˆÐ¸Ð±ÐºÐ°: {e}", exc_info=True)
                    conn.rollback()
            else:
                query, params, is_many = item
                if is_many: cursor.executemany(query, params)
                else: cursor.execute(query, params)
                conn.commit()
        except Exception as e:
            logger.error(f"DB-WORKER: ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}", exc_info=True)

def db_write_query(query, params=(), many=False):
    """ÐŸÐ¾Ð¼ÐµÑ‰Ð°ÐµÑ‚ Ð¾Ð´Ð¸Ð½Ð¾Ñ‡Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ð·Ð°Ð¿Ð¸ÑÑŒ Ð² Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ."""
    DB_WRITE_QUEUE.put((query, params, many))

def db_transaction(func: Callable[[sqlite3.Cursor], None]):
    """ÐŸÐ¾Ð¼ÐµÑ‰Ð°ÐµÑ‚ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð´Ð»Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸."""
    DB_WRITE_QUEUE.put(func)

def db_read_query(query, params=(), fetchone=False, fetchall=False):
    """Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚."""
    try:
        conn = sqlite3.connect(DB_NAME, timeout=10, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute(query, params)
        result = None
        if fetchone: result = cursor.fetchone()
        if fetchall: result = cursor.fetchall()
        conn.close()
        return result
    except Exception as e:
        logger.error(f"DB-READ: ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        return None

def init_db():
    """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð‘Ð” Ð² ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ñ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼Ð¸ v14."""
    db_write_query("CREATE TABLE IF NOT EXISTS users (user_id INTEGER PRIMARY KEY, first_name TEXT, username TEXT)")
    db_write_query("""
        CREATE TABLE IF NOT EXISTS cached_words (
            word_id INTEGER PRIMARY KEY AUTOINCREMENT,
            hebrew TEXT NOT NULL UNIQUE,
            normalized_hebrew TEXT NOT NULL,
            transcription TEXT,
            is_verb BOOLEAN,
            root TEXT,
            binyan TEXT,
            fetched_at TIMESTAMP
        )
    """)
    db_write_query("""
        CREATE TABLE IF NOT EXISTS translations (
            translation_id INTEGER PRIMARY KEY AUTOINCREMENT,
            word_id INTEGER,
            translation_text TEXT NOT NULL,
            context_comment TEXT,
            is_primary BOOLEAN NOT NULL,
            FOREIGN KEY (word_id) REFERENCES cached_words (word_id) ON DELETE CASCADE
        )
    """)
    db_write_query("""
        CREATE TABLE IF NOT EXISTS user_dictionary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER,
            word_id INTEGER,
            added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            srs_level INTEGER DEFAULT 0,
            next_review_at TIMESTAMP,
            FOREIGN KEY (user_id) REFERENCES users (user_id),
            FOREIGN KEY (word_id) REFERENCES cached_words (word_id) ON DELETE CASCADE,
            UNIQUE(user_id, word_id)
        )
    """)
    db_write_query("""
        CREATE TABLE IF NOT EXISTS verb_conjugations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            word_id INTEGER,
            tense TEXT,
            person TEXT,
            hebrew_form TEXT NOT NULL,
            normalized_hebrew_form TEXT NOT NULL,
            transcription TEXT,
            FOREIGN KEY (word_id) REFERENCES cached_words (word_id) ON DELETE CASCADE
        )
    """)
    # Ð˜Ð½Ð´ÐµÐºÑÑ‹ Ð´Ð»Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÐµÐ¹
    db_write_query("CREATE INDEX IF NOT EXISTS idx_normalized_hebrew ON cached_words(normalized_hebrew)")
    db_write_query("CREATE INDEX IF NOT EXISTS idx_normalized_hebrew_form ON verb_conjugations(normalized_hebrew_form)")
    db_write_query("CREATE INDEX IF NOT EXISTS idx_translations_word_id ON translations(word_id)")


# --- ÐœÐžÐ”Ð£Ð›Ð¬ÐÐÐ¯ ÐÐ Ð¥Ð˜Ð¢Ð•ÐšÐ¢Ð£Ð Ð ÐŸÐÐ Ð¡Ð•Ð Ð ---

def local_search(normalized_search_word: str) -> Optional[Dict[str, Any]]:
    """
    Ð˜Ñ‰ÐµÑ‚ ÑÐ»Ð¾Ð²Ð¾ Ð² Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾ ÐÐžÐ ÐœÐÐ›Ð˜Ð—ÐžÐ’ÐÐÐÐžÐ™ Ñ„Ð¾Ñ€Ð¼Ðµ.
    Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¸Ñ‰ÐµÑ‚ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ… Ð³Ð»Ð°Ð³Ð¾Ð»Ð¾Ð², Ð·Ð°Ñ‚ÐµÐ¼ Ð² ÐºÐ°Ð½Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ„Ð¾Ñ€Ð¼Ð°Ñ….
    """
    word_id = None
    # 1. ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼Ð°Ð¼ Ð³Ð»Ð°Ð³Ð¾Ð»Ð¾Ð²
    conjugation = db_read_query(
        "SELECT word_id FROM verb_conjugations WHERE normalized_hebrew_form = ?",
        (normalized_search_word,),
        fetchone=True
    )
    if conjugation:
        word_id = conjugation['word_id']
    else:
    # 2. ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ ÐºÐ°Ð½Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ñ„Ð¾Ñ€Ð¼Ð°Ð¼
        word_data_row = db_read_query("SELECT word_id FROM cached_words WHERE normalized_hebrew = ?", 
        (normalized_search_word,), fetchone=True)
        if word_data_row:
            word_id = word_data_row['word_id']

    if not word_id:
        return None

    word_data = db_read_query("SELECT * FROM cached_words WHERE word_id = ?", (word_id,), fetchone=True)
    if not word_data:
        return None
    
    translations = db_read_query("SELECT * FROM translations WHERE word_id = ? ORDER BY is_primary DESC", (word_id,), fetchall=True)
    
    result = dict(word_data)
    result['translations'] = [dict(t) for t in translations]
    return result

def parse_verb_page(soup: BeautifulSoup, main_header: Tag) -> Optional[Dict[str, Any]]:
    """ÐŸÐ°Ñ€ÑÐµÑ€ Ð´Ð»Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† Ð³Ð»Ð°Ð³Ð¾Ð»Ð¾Ð²."""
    logger.info("-> Ð—Ð°Ð¿ÑƒÑ‰ÐµÐ½ parse_verb_page.")
    try:
        data = {'is_verb': True}

        logger.info("--> parse_verb_page: ÐŸÐ¾Ð¸ÑÐº Ð¸Ð½Ñ„Ð¸Ð½Ð¸Ñ‚Ð¸Ð²Ð°...")
        infinitive_div = soup.find('div', id='INF-L')
        if not infinitive_div:
            logger.error("--> parse_verb_page: ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð±Ð»Ð¾Ðº Ð¸Ð½Ñ„Ð¸Ð½Ð¸Ñ‚Ð¸Ð²Ð° INF-L.")
            return None

        menukad_tag = infinitive_div.find('span', class_='menukad')
        if not menukad_tag:
            logger.error("--> parse_verb_page: ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ñ‚ÐµÐ³ menukad Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð±Ð»Ð¾ÐºÐ° Ð¸Ð½Ñ„Ð¸Ð½Ð¸Ñ‚Ð¸Ð²Ð°.")
            return None
        data['hebrew'] = menukad_tag.text.split('~')[0].strip()
        logger.info(f"--> parse_verb_page: Ð˜Ð½Ñ„Ð¸Ð½Ð¸Ñ‚Ð¸Ð² Ð½Ð°Ð¹Ð´ÐµÐ½: {data['hebrew']}")

        logger.info("--> parse_verb_page: ÐŸÐ¾Ð¸ÑÐº Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð°...")
        lead_div = soup.find('div', class_='lead')
        if not lead_div:
            logger.error(f"--> parse_verb_page Ð´Ð»Ñ '{data['hebrew']}': Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ 'div' Ñ ÐºÐ»Ð°ÑÑÐ¾Ð¼ 'lead' (Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´).")
            return None
        data['translations'] = parse_translations(lead_div.text.strip())
        if not data['translations']:
            logger.warning(f"--> parse_verb_page Ð´Ð»Ñ '{data['hebrew']}': Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ parse_translations Ð²ÐµÑ€Ð½ÑƒÐ»Ð° Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº.")
            return None
        logger.info(f"--> parse_verb_page: ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹.")

        logger.info("--> parse_verb_page: ÐŸÐ¾Ð¸ÑÐº Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸...")
        transcription_div = infinitive_div.find('div', class_='transcription')
        data['transcription'] = transcription_div.text.strip() if transcription_div else ''
        
        logger.info("--> parse_verb_page: ÐŸÐ¾Ð¸ÑÐº ÐºÐ¾Ñ€Ð½Ñ Ð¸ Ð±Ð¸Ð½ÑŒÑÐ½Ð°...")
        data['root'], data['binyan'] = None, None
        for p in main_header.find_next_siblings('p'):
            if 'Ð³Ð»Ð°Ð³Ð¾Ð»' in p.text.lower():
                binyan_tag = p.find('b')
                if binyan_tag: data['binyan'] = binyan_tag.text.strip()
            if 'ÐºÐ¾Ñ€ÐµÐ½ÑŒ' in p.text.lower():
                root_tag = p.find('span', class_='menukad')
                if root_tag: data['root'] = root_tag.text.strip()

        logger.info("--> parse_verb_page: ÐŸÐ¾Ð¸ÑÐº ÑÐ¿Ñ€ÑÐ¶ÐµÐ½Ð¸Ð¹...")
        conjugations = []
        verb_forms = soup.find_all('div', id=re.compile(r'^(AP|PERF|IMPF|IMP|INF)-'))
        tense_map = {'AP': 'Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐµ', 'PERF': 'Ð¿Ñ€Ð¾ÑˆÐµÐ´ÑˆÐµÐµ', 'IMPF': 'Ð±ÑƒÐ´ÑƒÑ‰ÐµÐµ', 'IMP': 'Ð¿Ð¾Ð²ÐµÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ', 'INF': 'Ð¸Ð½Ñ„Ð¸Ð½Ð¸Ñ‚Ð¸Ð²'}
        for form in verb_forms:
            form_id, menukad_tag, trans_tag = form.get('id'), form.find('span', class_='menukad'), form.find('div', class_='transcription')
            if all([form_id, menukad_tag, trans_tag]):
                tense_prefix = form_id.split('-')[0]
                person = form_id.split('-')[1] if len(form_id.split('-')) > 1 else "Ñ„Ð¾Ñ€Ð¼Ð°"
                conjugations.append({'tense': tense_map.get(tense_prefix), 'person': person, 'hebrew_form': menukad_tag.text.strip(), 'transcription': trans_tag.text.strip()})
        data['conjugations'] = conjugations
        logger.info(f"--> parse_verb_page: ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(conjugations)} Ñ„Ð¾Ñ€Ð¼ ÑÐ¿Ñ€ÑÐ¶ÐµÐ½Ð¸Ð¹.")

        logger.info("-> parse_verb_page Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾.")
        return data
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² parse_verb_page: {e}", exc_info=True)
        return None

def parse_noun_or_adjective_page(soup: BeautifulSoup, main_header: Tag) -> Optional[Dict[str, Any]]:
    """ÐŸÐ°Ñ€ÑÐµÑ€ Ð´Ð»Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¸ Ð¿Ñ€Ð¸Ð»Ð°Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ…."""
    logger.info("-> Ð—Ð°Ð¿ÑƒÑ‰ÐµÐ½ parse_noun_or_adjective_page.")
    try:
        data = {'is_verb': False, 'root': None, 'binyan': None, 'conjugations': []}
        
        logger.info("--> parse_noun_or_adjective_page: ÐŸÐ¾Ð¸ÑÐº ÐºÐ°Ð½Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ñ‹...")
        canonical_hebrew = None
        canonical_tag = main_header.find('span', class_='menukad')
        if canonical_tag:
            canonical_hebrew = canonical_tag.text.strip()
        elif soup.title and 'â€“' in soup.title.string:
            logger.info("--> parse_noun_or_adjective_page: Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ menukad, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð·Ð°Ð¿Ð°ÑÐ½Ð¾Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ (title).")
            potential_word = soup.title.string.split('â€“')[0].strip()
            if re.match(r'^[\u0590-\u05FF\s-]+$', potential_word):
                canonical_hebrew = potential_word

        if not canonical_hebrew:
            logger.error("--> parse_noun_or_adjective_page: ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ ÐºÐ°Ð½Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ñ„Ð¾Ñ€Ð¼Ñƒ.")
            return None
        data['hebrew'] = canonical_hebrew
        logger.info(f"--> parse_noun_or_adjective_page: ÐšÐ°Ð½Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ñ„Ð¾Ñ€Ð¼Ð° Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {data['hebrew']}")
        
        logger.info("--> parse_noun_or_adjective_page: ÐŸÐ¾Ð¸ÑÐº Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð°...")
        lead_div = soup.find('div', class_='lead')
        if not lead_div:
            logger.error(f"--> parse_noun_or_adjective_page Ð´Ð»Ñ '{data['hebrew']}': Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ 'div' Ñ ÐºÐ»Ð°ÑÑÐ¾Ð¼ 'lead' (Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´).")
            return None
        
        data['translations'] = parse_translations(lead_div.text.strip())
        if not data['translations']:
            logger.warning(f"--> parse_noun_or_adjective_page Ð´Ð»Ñ '{data['hebrew']}': Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ parse_translations Ð²ÐµÑ€Ð½ÑƒÐ»Ð° Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº.")
            return None
        logger.info(f"--> parse_noun_or_adjective_page: ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹.")

        logger.info("--> parse_noun_or_adjective_page: ÐŸÐ¾Ð¸ÑÐº Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸...")
        transcription_div = soup.find('div', class_='transcription')
        data['transcription'] = transcription_div.text.strip() if transcription_div else ''
        
        logger.info("-> parse_noun_or_adjective_page Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾.")
        return data
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² parse_noun_or_adjective_page: {e}", exc_info=True)
        return None

def fetch_and_cache_word_data(search_word: str) -> Tuple[str, Optional[Dict[str, Any]]]:
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ-Ð´Ð¸ÑÐ¿ÐµÑ‚Ñ‡ÐµÑ€ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°. ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÑ‚ Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ."""
    is_owner = False
    normalized_search_word = normalize_hebrew(search_word)
    with PARSING_EVENTS_LOCK:
        if normalized_search_word not in PARSING_EVENTS:
            PARSING_EVENTS[normalized_search_word] = threading.Event()
            is_owner = True
        event = PARSING_EVENTS[normalized_search_word]

    if not is_owner:
        logger.info(f"ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð»Ñ '{search_word}' ÑƒÐ¶Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½, Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ðµ...")
        event.wait(timeout=PARSING_TIMEOUT)
        logger.info(f"ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ '{search_word}' Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾, Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð² ÐºÑÑˆÐµ.")
        result = local_search(normalized_search_word)
        return ('ok', result) if result else ('not_found', None)

    try:
        logger.info(f"--- ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð´Ð»Ñ '{search_word}' ---")
        logger.info("Ð¨Ð°Ð³ 1: Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ HTTP-Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°...")
        try:
            search_url = f"https://www.pealim.com/ru/search/?q={quote(search_word)}"
            session = requests.Session()
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}
            session.headers.update(headers)
            search_response = session.get(search_url, timeout=10)
            search_response.raise_for_status()
            logger.info(f"Ð¨Ð°Ð³ 1.1: Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ {search_url}")
            
            if "/dict/" in search_response.url:
                response = search_response
                logger.info("Ð¨Ð°Ð³ 1.2: ÐŸÑ€ÑÐ¼Ð¾Ðµ Ð¿ÐµÑ€ÐµÐ½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð½Ð° ÑÐ»Ð¾Ð²Ð°Ñ€Ð½ÑƒÑŽ ÑÑ‚Ð°Ñ‚ÑŒÑŽ.")
            else:
                logger.info("Ð¨Ð°Ð³ 1.2: ÐžÑ‚Ð²ÐµÑ‚ - ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð¿Ð¾Ð¸ÑÐºÐ°, Ð¸Ñ‰ÐµÐ¼ Ð½ÑƒÐ¶Ð½ÑƒÑŽ ÑÑÑ‹Ð»ÐºÑƒ...")
                search_soup = BeautifulSoup(search_response.text, 'html.parser')
                results_container = search_soup.find('div', class_='results-by-verb') or search_soup.find('div', class_='results-by-meaning')
                if not results_container:
                    logger.warning(f"ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ Ð¿Ð¾Ð¸ÑÐºÐ° Ð´Ð»Ñ '{search_word}'.")
                    return 'not_found', None
                result_link = results_container.find('a', href=re.compile(r'/dict/'))
                if not result_link or not result_link.get('href'):
                    logger.warning(f"ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° ÑÐ»Ð¾Ð²Ð°Ñ€Ð½ÑƒÑŽ ÑÑ‚Ð°Ñ‚ÑŒÑŽ Ð´Ð»Ñ '{search_word}'.")
                    return 'not_found', None
                final_url = urljoin(search_response.url, result_link['href'])
                logger.info(f"Ð¨Ð°Ð³ 1.3: ÐÐ°Ð¹Ð´ÐµÐ½Ð° ÑÑÑ‹Ð»ÐºÐ°, Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´ Ð½Ð° {final_url}")
                response = session.get(final_url, timeout=10)
                response.raise_for_status()
        except requests.RequestException as e:
            logger.error(f"Ð¡ÐµÑ‚ÐµÐ²Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ Ðº pealim.com: {e}", exc_info=True)
            return 'error', None
        
        logger.info("Ð¨Ð°Ð³ 1.4: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°.")
        soup = BeautifulSoup(response.text, 'html.parser')
        
        logger.info("Ð¨Ð°Ð³ 2: ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ‚Ð¸Ð¿Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹...")
        main_header = soup.find('h2', class_='page-header')
        if not main_header:
            logger.error("ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð½Ðµ ÑƒÐ´Ð°Ð»ÑÑ: Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ 'h2' Ñ ÐºÐ»Ð°ÑÑÐ¾Ð¼ 'page-header'.")
            return 'error', None

        parsed_data = None
        if "ÑÐ¿Ñ€ÑÐ¶ÐµÐ½Ð¸Ðµ" in main_header.text.lower():
            logger.info("Ð¨Ð°Ð³ 2.1: Ð¡Ñ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð° ÐºÐ°Ðº Ð“Ð›ÐÐ“ÐžÐ›.")
            parsed_data = parse_verb_page(soup, main_header)
        else:
            logger.info("Ð¨Ð°Ð³ 2.1: Ð¡Ñ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð° ÐºÐ°Ðº Ð¡Ð£Ð©Ð•Ð¡Ð¢Ð’Ð˜Ð¢Ð•Ð›Ð¬ÐÐžÐ•/ÐŸÐ Ð˜Ð›ÐÐ“ÐÐ¢Ð•Ð›Ð¬ÐÐžÐ•.")
            parsed_data = parse_noun_or_adjective_page(soup, main_header)

        logger.info("Ð¨Ð°Ð³ 3: ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¸ ÐÐžÐ ÐœÐÐ›Ð˜Ð—ÐÐ¦Ð˜Ð¯ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°...")
        if not parsed_data:
            logger.error("ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð½Ðµ ÑƒÐ´Ð°Ð»ÑÑ: Ð¾Ð´Ð½Ð° Ð¸Ð· Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ (parse_verb_page/parse_noun_or_adjective_page) Ð²ÐµÑ€Ð½ÑƒÐ»Ð° None.")
            return 'error', None
        logger.info(f"Ð¨Ð°Ð³ 3.1: ÐŸÐ°Ñ€ÑÐµÑ€ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð²ÐµÑ€Ð½ÑƒÐ» Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ '{parsed_data['hebrew']}'.")
        parsed_data['normalized_hebrew'] = normalize_hebrew(parsed_data['hebrew'])
        if parsed_data.get('conjugations'):
            for conj in parsed_data['conjugations']:
                conj['normalized_hebrew_form'] = normalize_hebrew(conj['hebrew_form'])
        
        if local_search(parsed_data['normalized_hebrew']):
            logger.info(f"Ð¨Ð°Ð³ 3.2: ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ñ„Ð¾Ñ€Ð¼Ð° '{parsed_data['normalized_hebrew']}' ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ Ð² ÐºÑÑˆÐµ. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ.")
            return 'ok', local_search(parsed_data['normalized_hebrew'])

        logger.info(f"Ð¨Ð°Ð³ 4: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ '{parsed_data['hebrew']}' Ð¸ ÐµÐ³Ð¾ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ñ„Ð¾Ñ€Ð¼ Ð² Ð‘Ð”...")
        def _save_word_transaction(cursor: sqlite3.Cursor):
            cursor.execute("""
                INSERT INTO cached_words (hebrew, normalized_hebrew, transcription, is_verb, root, binyan, fetched_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                parsed_data['hebrew'], parsed_data['normalized_hebrew'], parsed_data['transcription'],
                parsed_data['is_verb'], parsed_data.get('root'), parsed_data.get('binyan'), datetime.now()
            ))
            word_id = cursor.lastrowid
            
            if word_id and parsed_data.get('translations'):
                translations_to_insert = [
                    (word_id, t['translation_text'], t['context_comment'], t['is_primary'])
                    for t in parsed_data['translations']
                ]
                cursor.executemany("""
                    INSERT INTO translations (word_id, translation_text, context_comment, is_primary)
                    VALUES (?, ?, ?, ?)
                """, translations_to_insert)

            if word_id and parsed_data.get('conjugations'):
                conjugations_to_insert = [
                    (word_id, c['tense'], c['person'], c['hebrew_form'], c['normalized_hebrew_form'], c['transcription'])
                    for c in parsed_data['conjugations']
                ]
                cursor.executemany("""
                    INSERT INTO verb_conjugations 
                    (word_id, tense, person, hebrew_form, normalized_hebrew_form, transcription)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, conjugations_to_insert)
        db_transaction(_save_word_transaction)
        logger.info("Ð¨Ð°Ð³ 4.1: Ð¢Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ñ Ð½Ð° Ð·Ð°Ð¿Ð¸ÑÑŒ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð° Ð² Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ.")
        logger.info("Ð¨Ð°Ð³ 5: ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð²Ð° Ð² Ð‘Ð” Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°...")
        final_word_data = None
        for i in range(DB_READ_ATTEMPTS):
            logger.info(f"Ð¨Ð°Ð³ 5.{i+1}: ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð¸Ð· Ð‘Ð”...")
            final_word_data = local_search(parsed_data['normalized_hebrew'])
            if final_word_data:
                logger.info("Ð¨Ð°Ð³ 5.x: Ð¡Ð»Ð¾Ð²Ð¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð² Ð‘Ð”.")
                break
            time.sleep(DB_READ_DELAY)
        
        if final_word_data:
            logger.info(f"--- ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð»Ñ '{search_word}' Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ Ð£Ð¡ÐŸÐ•Ð¨ÐÐž. ---")
            return ('ok', final_word_data)
        else:
            logger.error(f"--- ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð»Ñ '{search_word}' Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ Ñ ÐžÐ¨Ð˜Ð‘ÐšÐžÐ™ Ð‘Ð” (Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð·Ð°Ð¿Ð¸ÑÑŒ). ---")
            return ('db_error', None)
            
    except Exception as e:
        logger.error(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð² fetch_and_cache_word_data: {e}", exc_info=True)
        return 'error', None
    finally:
        logger.info(f"Ð¨Ð°Ð³ 6: ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð»Ñ '{search_word}'.")
        with PARSING_EVENTS_LOCK:
            if normalized_search_word in PARSING_EVENTS:
                PARSING_EVENTS[normalized_search_word].set()
                del PARSING_EVENTS[normalized_search_word]

# --- ÐšÐžÐ›Ð›Ð‘Ð­Ðš-Ð”ÐÐÐÐ«Ð• Ð˜ Ð¡ÐžÐ¡Ð¢ÐžÐ¯ÐÐ˜Ð¯ ---
TRAINING_MENU_STATE, FLASHCARD_SHOW, FLASHCARD_EVAL, AWAITING_VERB_ANSWER, VERB_TRAINER_NEXT_ACTION = range(5)
CB_DICT_VIEW, CB_DICT_DELETE_MODE, CB_DICT_CONFIRM_DELETE, CB_DICT_EXECUTE_DELETE = "d_v", "d_dm", "d_cd", "d_ed"
CB_ADD, CB_SHOW_VERB, CB_VIEW_CARD = "add", "sh_v", "v_c"
CB_TRAIN_MENU, CB_TRAIN_HE_RU, CB_TRAIN_RU_HE, CB_VERB_TRAINER_START = "t_m", "t_hr", "t_rh", "vts"
CB_SHOW_ANSWER, CB_EVAL_CORRECT, CB_EVAL_INCORRECT, CB_END_TRAINING = "sh_a", "e_c", "e_i", "e_t"

# --- ÐžÐ¡ÐÐžÐ’ÐÐ«Ð• Ð¥Ð•ÐÐ”Ð›Ð•Ð Ð« ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    db_write_query("INSERT OR IGNORE INTO users (user_id, first_name, username) VALUES (?, ?, ?)", (user.id, user.first_name, user.username))
    keyboard = [[InlineKeyboardButton("ðŸ§  ÐœÐ¾Ð¹ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ", callback_data=f"{CB_DICT_VIEW}_0")], [InlineKeyboardButton("ðŸ’ª Ð¢Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ°", callback_data=CB_TRAIN_MENU)]]
    await update.message.reply_text(f"ÐŸÑ€Ð¸Ð²ÐµÑ‚, {user.first_name}! ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒ Ð¼Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾ Ð½Ð° Ð¸Ð²Ñ€Ð¸Ñ‚Ðµ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°.", reply_markup=InlineKeyboardMarkup(keyboard))

async def main_menu(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    keyboard = [[InlineKeyboardButton("ðŸ§  ÐœÐ¾Ð¹ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ", callback_data=f"{CB_DICT_VIEW}_0")], [InlineKeyboardButton("ðŸ’ª Ð¢Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ°", callback_data=CB_TRAIN_MENU)]]
    await query.edit_message_text("Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ:", reply_markup=InlineKeyboardMarkup(keyboard))

async def display_word_card(
    context: ContextTypes.DEFAULT_TYPE,
    user_id: int,
    chat_id: int,
    word_data: dict,
    message_id: Optional[int] = None,
    in_dictionary: Optional[bool] = None
):
    """
    ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶Ð°ÐµÑ‚ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÑƒ ÑÐ»Ð¾Ð²Ð°. Ð ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€ÑƒÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ, ÐµÑÐ»Ð¸
    Ð¿ÐµÑ€ÐµÐ´Ð°Ð½ message_id, Ð¸Ð½Ð°Ñ‡Ðµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ð¾Ðµ.
    """
    word_id = word_data['word_id']
    
    if in_dictionary is None:
        in_dictionary = db_read_query("SELECT 1 FROM user_dictionary WHERE user_id = ? AND word_id = ?", (user_id, word_id), fetchone=True)
    
    translations = word_data.get('translations', [])
    primary_translation = next((t['translation_text'] for t in translations if t['is_primary']), "ÐŸÐµÑ€ÐµÐ²Ð¾Ð´ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
    other_translations = [t['translation_text'] for t in translations if not t['is_primary']]
    
    translation_str = primary_translation
    if other_translations:
        translation_str += f" (Ñ‚Ð°ÐºÐ¶Ðµ: {', '.join(other_translations)})"

    card_text_header = f"Ð¡Ð»Ð¾Ð²Ð¾ *{word_data['hebrew']}* ÑƒÐ¶Ðµ Ð² Ð²Ð°ÑˆÐµÐ¼ ÑÐ»Ð¾Ð²Ð°Ñ€Ðµ." if in_dictionary else f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾: *{word_data['hebrew']}*"
    card_text = f"{card_text_header} [{word_data.get('transcription', '')}]\nÐŸÐµÑ€ÐµÐ²Ð¾Ð´: {translation_str}"

    keyboard_buttons = []
    if in_dictionary:
        keyboard_buttons.append(InlineKeyboardButton("ðŸ—‘ï¸ Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ", callback_data=f"{CB_DICT_CONFIRM_DELETE}_{word_id}_0"))
    else:
        keyboard_buttons.append(InlineKeyboardButton("âž• Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ", callback_data=f"{CB_ADD}_{word_id}"))

    if word_data.get('is_verb'):
        keyboard_buttons.append(InlineKeyboardButton("ðŸ“– Ð¡Ð¿Ñ€ÑÐ¶ÐµÐ½Ð¸Ñ", callback_data=f"{CB_SHOW_VERB}_{word_id}"))

    keyboard = [keyboard_buttons, [InlineKeyboardButton("â¬…ï¸ Ð’ Ð³Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")]]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    try:
        if message_id:
            await context.bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=card_text, reply_markup=reply_markup, parse_mode=ParseMode.MARKDOWN)
        else:
            await context.bot.send_message(chat_id=chat_id, text=card_text, reply_markup=reply_markup, parse_mode=ParseMode.MARKDOWN)
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐµ/Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸ ÑÐ»Ð¾Ð²Ð°: {e}", exc_info=True)

async def handle_text_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message or not update.message.text: return
    text = update.message.text.strip()
    user_id = update.effective_user.id
    chat_id = update.effective_chat.id

    if not re.match(r'^[\u0590-\u05FF\s-]+$', text):
        await update.message.reply_text("ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð±ÑƒÐºÐ²Ñ‹ Ð¸Ð²Ñ€Ð¸Ñ‚Ð°, Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹ Ð¸ Ð´ÐµÑ„Ð¸ÑÑ‹.")
        return
    if len(text.split()) > 1:
        await update.message.reply_text("ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐ¹Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ ÑÐ»Ð¾Ð²Ñƒ Ð·Ð° Ñ€Ð°Ð·.")
        return

    normalized_text = normalize_hebrew(text)
    word_data = local_search(normalized_text)

    if word_data:
        await display_word_card(context, user_id, chat_id, word_data)
        return

    status_message = await update.message.reply_text("ðŸ”Ž Ð˜Ñ‰Ñƒ ÑÐ»Ð¾Ð²Ð¾ Ð²Ð¾ Ð²Ð½ÐµÑˆÐ½ÐµÐ¼ ÑÐ»Ð¾Ð²Ð°Ñ€Ðµ...")
    status, data = await asyncio.to_thread(fetch_and_cache_word_data, text)

    if status == 'ok' and data:
        await display_word_card(context, user_id, chat_id, data, message_id=status_message.message_id)
    elif status == 'not_found':
        await context.bot.edit_message_text(f"Ð¡Ð»Ð¾Ð²Ð¾ '{text}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾.", chat_id=chat_id, message_id=status_message.message_id)
    elif status == 'error':
        await context.bot.edit_message_text("Ð’Ð½ÐµÑˆÐ½Ð¸Ð¹ ÑÐµÑ€Ð²Ð¸Ñ ÑÐ»Ð¾Ð²Ð°Ñ€Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ, Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¿Ð¾Ð·Ð¶Ðµ.", chat_id=chat_id, message_id=status_message.message_id)
    elif status == 'db_error':
        await context.bot.edit_message_text("ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ ÑÐ»Ð¾Ð²Ð°. ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿Ð¾Ð·Ð¶Ðµ.", chat_id=chat_id, message_id=status_message.message_id)

async def add_word_to_dictionary(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    word_id = int(query.data.split('_')[1])
    user_id = query.from_user.id
    
    db_write_query("INSERT OR IGNORE INTO user_dictionary (user_id, word_id, next_review_at) VALUES (?, ?, ?)", (user_id, word_id, datetime.now()))
    
    await query.answer("Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾!")

    word_data_row = db_read_query("SELECT normalized_hebrew FROM cached_words WHERE word_id = ?", (word_id,), fetchone=True)
    if word_data_row:
        word_data = local_search(word_data_row['normalized_hebrew'])
        if word_data:
            await display_word_card(context, user_id, query.message.chat_id, word_data, message_id=query.message.message_id, in_dictionary=True)

async def view_dictionary_page_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    parts = query.data.split('_')
    page = int(parts[-1])
    deletion_mode = parts[1] == "dm"
    await view_dictionary_page_logic(update, context, page=page, deletion_mode=deletion_mode)

async def view_dictionary_page_logic(update: Update, context: ContextTypes.DEFAULT_TYPE, page: int, deletion_mode: bool, exclude_word_id: Optional[int] = None):
    query = update.callback_query
    user_id = query.from_user.id
    
    sql_query = """
        SELECT cw.word_id, cw.hebrew, t.translation_text
        FROM cached_words cw
        JOIN user_dictionary ud ON cw.word_id = ud.word_id
        JOIN translations t ON cw.word_id = t.word_id
        WHERE ud.user_id = ? AND t.is_primary = 1
        ORDER BY ud.added_at DESC LIMIT 6 OFFSET ?
    """
    words_from_db = db_read_query(sql_query, (user_id, page * 5), fetchall=True)
    
    words = [w for w in words_from_db if w['word_id'] != exclude_word_id] if exclude_word_id else words_from_db
    
    has_next_page = len(words) > 5
    words = words[:5]

    if not words and page > 0:
        return await view_dictionary_page_logic(update, context, page=page - 1, deletion_mode=False)
    if not words and page == 0:
        await query.edit_message_text("Ð’Ð°Ñˆ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð¿ÑƒÑÑ‚.", reply_markup=InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ Ð’ Ð³Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")]]))
        return

    keyboard, message_text = [], "Ð’Ð°Ñˆ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ (ÑÑ‚Ñ€. {}):\n\n".format(page + 1)
    if deletion_mode: message_text = "Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÑÐ»Ð¾Ð²Ð¾ Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ:"
    for word in words:
        if deletion_mode:
            keyboard.append([InlineKeyboardButton(f"ðŸ—‘ï¸ {word['hebrew']}", callback_data=f"{CB_DICT_CONFIRM_DELETE}_{word['word_id']}_{page}")])
        else:
            message_text += f"â€¢ {word['hebrew']} â€” {word['translation_text']}\n"
    
    nav_buttons = []
    nav_pattern = CB_DICT_DELETE_MODE if deletion_mode else CB_DICT_VIEW
    if page > 0: nav_buttons.append(InlineKeyboardButton("â—€ï¸", callback_data=f"{nav_pattern}_{page-1}"))
    if has_next_page: nav_buttons.append(InlineKeyboardButton("â–¶ï¸", callback_data=f"{nav_pattern}_{page+1}"))
    if nav_buttons: keyboard.append(nav_buttons)
    
    if deletion_mode:
        keyboard.append([InlineKeyboardButton("â¬…ï¸ Ðš ÑÐ»Ð¾Ð²Ð°Ñ€ÑŽ", callback_data=f"{CB_DICT_VIEW}_{page}")])
    else:
        keyboard.append([InlineKeyboardButton("ðŸ—‘ï¸ Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÐ»Ð¾Ð²Ð¾", callback_data=f"{CB_DICT_DELETE_MODE}_0")])
        keyboard.append([InlineKeyboardButton("â¬…ï¸ Ð’ Ð³Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")])
    
    await query.edit_message_text(message_text, reply_markup=InlineKeyboardMarkup(keyboard))

async def confirm_delete_word(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    _, _, word_id_str, page_str = query.data.split('_')
    word_data = db_read_query("SELECT hebrew FROM cached_words WHERE word_id = ?", (word_id_str,), fetchone=True)
    if not word_data:
        await query.edit_message_text("ÐžÑˆÐ¸Ð±ÐºÐ°: ÑÐ»Ð¾Ð²Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾.")
        return
    text = f"Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÐ»Ð¾Ð²Ð¾ '{word_data['hebrew']}'?"
    keyboard = [[InlineKeyboardButton("âœ… Ð”Ð°", callback_data=f"{CB_DICT_EXECUTE_DELETE}_{word_id_str}_{page_str}")], [InlineKeyboardButton("âŒ ÐÐµÑ‚", callback_data=f"{CB_DICT_DELETE_MODE}_{page_str}")]]
    await query.edit_message_text(text, reply_markup=InlineKeyboardMarkup(keyboard))

async def execute_delete_word(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer("Ð¡Ð»Ð¾Ð²Ð¾ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾")
    _, _, word_id_str, page_str = query.data.split('_')
    word_id, page = int(word_id_str), int(page_str)
    
    db_write_query("DELETE FROM user_dictionary WHERE user_id = ? AND word_id = ?", (query.from_user.id, word_id))
    
    await view_dictionary_page_logic(update, context, page=page, deletion_mode=False, exclude_word_id=word_id)

async def training_menu(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    if query: 
        await query.answer()
        # ÐŸÑ€Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‚Ðµ Ð² Ð¼ÐµÐ½ÑŽ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ðº, Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
        await query.edit_message_text(
            "Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ñ€ÐµÐ¶Ð¸Ð¼ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸:", 
            reply_markup=InlineKeyboardMarkup([
                [InlineKeyboardButton("ðŸ‡®ðŸ‡± â†’ ðŸ‡·ðŸ‡º", callback_data=CB_TRAIN_HE_RU)], 
                [InlineKeyboardButton("ðŸ‡·ðŸ‡º â†’ ðŸ‡®ðŸ‡±", callback_data=CB_TRAIN_RU_HE)], 
                [InlineKeyboardButton("ðŸ”¥ Ð“Ð»Ð°Ð³Ð¾Ð»Ñ‹", callback_data=CB_VERB_TRAINER_START)], 
                [InlineKeyboardButton("â¬…ï¸ Ð’ Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")]
            ])
        )
    return TRAINING_MENU_STATE

async def start_flashcard_training(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    context.user_data['training_mode'] = query.data
    
    sql_query = """
        SELECT cw.*, t.translation_text
        FROM cached_words cw
        JOIN user_dictionary ud ON cw.word_id = ud.word_id
        JOIN translations t ON cw.word_id = t.word_id
        WHERE ud.user_id = ? AND cw.is_verb = 0 AND t.is_primary = 1
        ORDER BY ud.next_review_at ASC LIMIT 10
    """
    words = db_read_query(sql_query, (query.from_user.id,), fetchall=True)

    if not words:
        await query.edit_message_text("Ð’ ÑÐ»Ð¾Ð²Ð°Ñ€Ðµ Ð½ÐµÑ‚ ÑÐ»Ð¾Ð² Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸.", reply_markup=InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ ÐÐ°Ð·Ð°Ð´", callback_data=CB_TRAIN_MENU)]]))
        return ConversationHandler.END
    context.user_data.update({'words': [dict(w) for w in words], 'idx': 0, 'correct': 0})
    return await show_next_card(update, context)

async def show_next_card(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    if query: await query.answer()
    idx, words = context.user_data['idx'], context.user_data['words']
    if idx >= len(words):
        await query.edit_message_text(f"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {context.user_data['correct']}/{len(words)}", reply_markup=InlineKeyboardMarkup([[InlineKeyboardButton("ðŸ’ª Ð•Ñ‰Ðµ", callback_data=context.user_data['training_mode'])], [InlineKeyboardButton("â¬…ï¸ Ð’ Ð¼ÐµÐ½ÑŽ", callback_data=CB_TRAIN_MENU)]]))
        return ConversationHandler.END
    word = words[idx]
    question = word['hebrew'] if context.user_data['training_mode'] == CB_TRAIN_HE_RU else word['translation_text']
    keyboard = [[InlineKeyboardButton("ðŸ’¡ ÐžÑ‚Ð²ÐµÑ‚", callback_data=CB_SHOW_ANSWER)], [InlineKeyboardButton("âŒ Ð—Ð°ÐºÐ¾Ð½Ñ‡Ð¸Ñ‚ÑŒ", callback_data=CB_END_TRAINING)]]
    
    message_text = f"Ð¡Ð»Ð¾Ð²Ð¾ {idx+1}/{len(words)}:\n\n*{question}*"
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    if query:
        await query.edit_message_text(text=message_text, reply_markup=reply_markup, parse_mode=ParseMode.MARKDOWN)
    
    return FLASHCARD_SHOW

async def show_answer(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    word = context.user_data['words'][context.user_data['idx']]
    answer_text = f"*{word['hebrew']}* [{word['transcription']}]\nÐŸÐµÑ€ÐµÐ²Ð¾Ð´: {word['translation_text']}"
    keyboard = [[InlineKeyboardButton("âœ… Ð—Ð½Ð°ÑŽ", callback_data=CB_EVAL_CORRECT)], [InlineKeyboardButton("âŒ ÐÐµ Ð·Ð½Ð°ÑŽ", callback_data=CB_EVAL_INCORRECT)]]
    await query.edit_message_text(answer_text, reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN)
    return FLASHCARD_EVAL

async def handle_self_evaluation(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    word = context.user_data['words'][context.user_data['idx']]
    srs_data = db_read_query("SELECT srs_level FROM user_dictionary WHERE user_id = ? AND word_id = ?", (query.from_user.id, word['word_id']), fetchone=True)
    srs_level = srs_data['srs_level'] if srs_data else 0
    if query.data == CB_EVAL_CORRECT:
        context.user_data['correct'] += 1
        srs_level += 1
    else: srs_level = 0
    next_review_date = datetime.now() + timedelta(days=[0, 1, 3, 7, 14, 30, 90][min(srs_level, 6)])
    db_write_query("UPDATE user_dictionary SET srs_level = ?, next_review_at = ? WHERE user_id = ? AND word_id = ?", (srs_level, next_review_date, query.from_user.id, word['word_id']))
    context.user_data['idx'] += 1
    return await show_next_card(update, context)

async def start_verb_trainer(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    
    user_id = query.from_user.id
    verb, conjugation = None, None

    for i in range(VERB_TRAINER_RETRY_ATTEMPTS):
        verb_candidate = db_read_query("SELECT cw.* FROM cached_words cw JOIN user_dictionary ud ON cw.word_id = ud.word_id WHERE ud.user_id = ? AND cw.is_verb = 1 ORDER BY RANDOM() LIMIT 1", (user_id,), fetchone=True)
        if not verb_candidate:
            await query.edit_message_text("Ð’ Ð²Ð°ÑˆÐµÐ¼ ÑÐ»Ð¾Ð²Ð°Ñ€Ðµ Ð½ÐµÑ‚ Ð³Ð»Ð°Ð³Ð¾Ð»Ð¾Ð² Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸.", reply_markup=InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ ÐÐ°Ð·Ð°Ð´", callback_data=CB_TRAIN_MENU)]]))
            return TRAINING_MENU_STATE

        conjugation_candidate = db_read_query("SELECT * FROM verb_conjugations WHERE word_id = ? ORDER BY RANDOM() LIMIT 1", (verb_candidate['word_id'],), fetchone=True)
        if conjugation_candidate:
            verb, conjugation = verb_candidate, conjugation_candidate
            break
        else:
            logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…: Ñƒ Ð³Ð»Ð°Ð³Ð¾Ð»Ð° {verb_candidate['hebrew']} (id: {verb_candidate['word_id']}) Ð½ÐµÑ‚ ÑÐ¿Ñ€ÑÐ¶ÐµÐ½Ð¸Ð¹. ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° {i+1}/{VERB_TRAINER_RETRY_ATTEMPTS}")

    if not verb or not conjugation:
        await query.edit_message_text("Ð’Ð¾Ð·Ð½Ð¸ÐºÐ»Ð° Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸.", reply_markup=InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ ÐÐ°Ð·Ð°Ð´", callback_data=CB_TRAIN_MENU)]]))
        return TRAINING_MENU_STATE

    context.user_data['answer'] = dict(conjugation)
    context.user_data['answer']['normalized_hebrew_form'] = normalize_hebrew(conjugation['hebrew_form'])
    
    await query.edit_message_text(f"Ð“Ð»Ð°Ð³Ð¾Ð»: *{verb['hebrew']}*\nÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ñ„Ð¾Ñ€Ð¼Ñƒ Ð´Ð»Ñ: *{conjugation['tense']}, {conjugation['person']}*", parse_mode=ParseMode.MARKDOWN)
    return AWAITING_VERB_ANSWER

async def check_verb_answer(update: Update, context: ContextTypes.DEFAULT_TYPE):
    correct = context.user_data['answer']
    
    normalized_user_answer = normalize_hebrew(update.message.text)
    correct_normalized_form = correct['normalized_hebrew_form']

    if normalized_user_answer == correct_normalized_form:
        await update.message.reply_text(f"âœ… Ð’ÐµÑ€Ð½Ð¾! *{correct['hebrew_form']}*", parse_mode=ParseMode.MARKDOWN)
    else:
        await update.message.reply_text(f"âŒ ÐÐµÐ²ÐµÑ€Ð½Ð¾. ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾: *{correct['hebrew_form']}*", parse_mode=ParseMode.MARKDOWN)
    
    db_write_query("UPDATE user_dictionary SET next_review_at = ? WHERE user_id = ? AND word_id = ?", (datetime.now() + timedelta(days=1), update.effective_user.id, correct['word_id']))
    
    keyboard = [[InlineKeyboardButton("ðŸ”¥ ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ", callback_data=CB_VERB_TRAINER_START)], [InlineKeyboardButton("â¬…ï¸ Ð’ Ð¼ÐµÐ½ÑŽ", callback_data=CB_TRAIN_MENU)]]
    await update.message.reply_text("Ð§Ñ‚Ð¾ Ð´Ð°Ð»ÑŒÑˆÐµ?", reply_markup=InlineKeyboardMarkup(keyboard))
    
    return VERB_TRAINER_NEXT_ACTION

async def end_training(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    await query.edit_message_text("Ð¢Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ñ€ÐµÑ€Ð²Ð°Ð½Ð°.")
    await training_menu(update, context)
    return ConversationHandler.END

async def back_to_main_menu(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await main_menu(update, context)
    return ConversationHandler.END

async def show_verb_conjugations(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    word_id = int(query.data.split('_')[-1])
    word_info = db_read_query("SELECT hebrew FROM cached_words WHERE word_id = ?", (word_id,), fetchone=True)
    conjugations_raw = db_read_query("SELECT tense, person, hebrew_form, transcription FROM verb_conjugations WHERE word_id = ? ORDER BY id", (word_id,), fetchall=True)
    
    keyboard = [[InlineKeyboardButton("â¬…ï¸ ÐÐ°Ð·Ð°Ð´ Ðº ÑÐ»Ð¾Ð²Ñƒ", callback_data=f"{CB_VIEW_CARD}_{word_id}")]]

    if not conjugations_raw or not word_info:
        await query.edit_message_text("Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð³Ð»Ð°Ð³Ð¾Ð»Ð° Ð½ÐµÑ‚ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ ÑÐ¿Ñ€ÑÐ¶ÐµÐ½Ð¸Ð¹.", reply_markup=InlineKeyboardMarkup(keyboard))
        return
        
    conjugations_by_tense = {}
    message_text = f"Ð¡Ð¿Ñ€ÑÐ¶ÐµÐ½Ð¸Ñ Ð´Ð»Ñ *{word_info['hebrew']}*:\n"
    
    for conj in conjugations_raw:
        if conj['tense'] not in conjugations_by_tense: conjugations_by_tense[conj['tense']] = []
        conjugations_by_tense[conj['tense']].append(conj)
        
    for tense, conjugations in conjugations_by_tense.items():
        message_text += f"\n*{tense.capitalize()}*:\n"
        for conj in conjugations: message_text += f"_{conj['person']}_: {conj['hebrew_form']} ({conj['transcription']})\n"
            
    if len(message_text) > 4096: message_text = message_text[:4090] + "\n(...)"
    
    await query.edit_message_text(message_text, reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN)

async def view_word_card_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸ ÑÐ»Ð¾Ð²Ð° Ð¿Ð¾ ÐµÐ³Ð¾ ID."""
    query = update.callback_query
    await query.answer()
    word_id = int(query.data.split('_')[-1])
    user_id = query.from_user.id
    chat_id = query.message.chat_id
    message_id = query.message.message_id
    
    word_data = db_read_query("SELECT * FROM cached_words WHERE word_id = ?", (word_id,), fetchone=True)
    if word_data:
        await display_word_card(context, user_id, chat_id, dict(word_data), message_id=message_id)
    else:
        await query.edit_message_text("ÐžÑˆÐ¸Ð±ÐºÐ°: ÑÐ»Ð¾Ð²Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾.", reply_markup=InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ Ð’ Ð³Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")]]))


def main() -> None:
    if not BOT_TOKEN:
        logger.critical("Ð¢Ð¾ÐºÐµÐ½ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½. Ð£ÐºÐ°Ð¶Ð¸Ñ‚Ðµ TELEGRAM_BOT_TOKEN Ð² .env Ñ„Ð°Ð¹Ð»Ðµ.")
        sys.exit("Ð¢Ð¾ÐºÐµÐ½ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½.")

    db_worker_thread = threading.Thread(target=db_worker, daemon=True)
    db_worker_thread.start()
    init_db()
    
    application = Application.builder().token(BOT_TOKEN).build()

    conv_defaults = {"per_user": True, "per_chat": True, "conversation_timeout": CONVERSATION_TIMEOUT_SECONDS}

    training_conv = ConversationHandler(
        entry_points=[CallbackQueryHandler(training_menu, pattern=f"^{CB_TRAIN_MENU}$")],
        states={
            TRAINING_MENU_STATE: [
                CallbackQueryHandler(start_flashcard_training, pattern=f"^({CB_TRAIN_HE_RU}|{CB_TRAIN_RU_HE})$"),
                CallbackQueryHandler(start_verb_trainer, pattern=f"^{CB_VERB_TRAINER_START}$"),
                CallbackQueryHandler(back_to_main_menu, pattern="^main_menu$")
            ],
            FLASHCARD_SHOW: [
                CallbackQueryHandler(show_answer, pattern=f"^{CB_SHOW_ANSWER}$"),
                CallbackQueryHandler(end_training, pattern=f"^{CB_END_TRAINING}$")
            ],
            FLASHCARD_EVAL: [
                CallbackQueryHandler(handle_self_evaluation, pattern=f"^{CB_EVAL_CORRECT}|{CB_EVAL_INCORRECT}$")
            ],
            AWAITING_VERB_ANSWER: [
                MessageHandler(filters.TEXT & ~filters.COMMAND, check_verb_answer)
            ],
            VERB_TRAINER_NEXT_ACTION: [
                CallbackQueryHandler(start_verb_trainer, pattern=f"^{CB_VERB_TRAINER_START}$"),
                CallbackQueryHandler(training_menu, pattern=f"^{CB_TRAIN_MENU}$")
            ]
        },
        fallbacks=[
            CallbackQueryHandler(end_training, pattern=f"^{CB_END_TRAINING}$"),
            CallbackQueryHandler(back_to_main_menu, pattern="^main_menu$"),
        ],
        map_to_parent={
            ConversationHandler.END: TRAINING_MENU_STATE
        },
        **conv_defaults
    )

    application.add_handler(CommandHandler("start", start))
    application.add_handler(CallbackQueryHandler(main_menu, pattern="^main_menu$"))
    application.add_handler(CallbackQueryHandler(view_dictionary_page_handler, pattern=f"^{CB_DICT_VIEW}_|{CB_DICT_DELETE_MODE}_"))
    application.add_handler(CallbackQueryHandler(confirm_delete_word, pattern=f"^{CB_DICT_CONFIRM_DELETE}_"))
    application.add_handler(CallbackQueryHandler(execute_delete_word, pattern=f"^{CB_DICT_EXECUTE_DELETE}_"))
    application.add_handler(CallbackQueryHandler(add_word_to_dictionary, pattern=f"^{CB_ADD}_"))
    application.add_handler(CallbackQueryHandler(show_verb_conjugations, pattern=f"^{CB_SHOW_VERB}_"))
    application.add_handler(CallbackQueryHandler(view_word_card_handler, pattern=f"^{CB_VIEW_CARD}_"))
    application.add_handler(training_conv)
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text_message))

    logger.info("Ð‘Ð¾Ñ‚ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ÑÑ...")
    application.run_polling()
    
    DB_WRITE_QUEUE.put(None)
    db_worker_thread.join()

if __name__ == "__main__":
    main()
